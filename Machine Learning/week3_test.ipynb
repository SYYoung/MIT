{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(Z):\n",
    "    t2 = np.apply_along_axis(lambda m: m if m > 0 else 0, 1, Z)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZ(X, W):\n",
    "    return np.matmul(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(fZ, V):\n",
    "    return np.matmul(V, fZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([1,0,-1,5]).reshape(4,1)\n",
    "np.apply_along_axis(lambda x: x if x > 0 else 0, 1, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutput(U):\n",
    "    fu = f(U)\n",
    "    t1 = np.exp(fu)\n",
    "    t1 = t1/(np.sum(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  0, -1],\n",
       "        [ 0,  1, -1],\n",
       "        [-1,  0, -1],\n",
       "        [ 0, -1, -1]]),\n",
       " array([[ 1,  1,  1,  1,  0],\n",
       "        [-1, -1, -1, -1,  2]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([[1, 0, -1], [0, 1, -1], [-1,0,-1],[0,-1,-1]])\n",
    "v = np.array([[1,1,1,1,0],[-1,-1,-1,-1,2]])\n",
    "w,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [14],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3,14,1]).reshape(3,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  2],\n",
       "        [ 13],\n",
       "        [ -4],\n",
       "        [-15]]),\n",
       " array([[ 2],\n",
       "        [13],\n",
       "        [ 0],\n",
       "        [ 0]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = getZ(x,w)\n",
    "fz = f(z)\n",
    "z,fz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]),\n",
       " (5, 1),\n",
       " (4, 1))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = np.array([[0,1,1,1]])\n",
    "k1 = np.append(k1,[1]).reshape(5,1)\n",
    "k1,k1.shape, fz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [-1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u12 = u(k1, v)\n",
    "u12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = f(u12)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-1*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_1(Wh, Wx, H, X, B):\n",
    "    t1 = np.matmul(Wh, H) + np.matmul(Wx, X) + B\n",
    "    return sigmoid(t1)\n",
    "\n",
    "def ft_2(Wh, Wx, H, X, B):\n",
    "    t1 = Wh * H + Wx * X + B\n",
    "    return sigmoid(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999983298578152"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_2(1, 2, 3, 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getc(F, C, I, Wh, Wx, H, X, B):\n",
    "    t1 = Wh * H + Wx * X + B\n",
    "    ct = F * C + I * np.tanh(t1)\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geth(OT, CT):\n",
    "    return OT* np.tanh(CT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fh = 0 \n",
    "w_fx = 0 \n",
    "bf = -100\n",
    "w_ch = -100\n",
    "w_ih = 0 \n",
    "w_ix = 100 \n",
    "bi = 100 \n",
    "w_cx = 50\n",
    "w_oh=0 \n",
    "w_ox = 100 \n",
    "bo = 0 \n",
    "bc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "h = 0\n",
    "c = 0\n",
    "x_all = np.array([0,0,1,1,1,0])\n",
    "\n",
    "def cal(c, h, x):\n",
    "    f_t = ft_2(w_fh, w_fx, h, x, bf)\n",
    "    i_t = ft_2(w_ih, w_ix, h, x, bi)\n",
    "    o_t = ft_2(w_oh, w_ox, h, x, bo)\n",
    "    ct = getc(f_t, c, i_t, w_ch, w_cx, h, x, bc)\n",
    "    ht = geth(o_t, ct)\n",
    "    return ct, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "c, h = cal(c,h,x_all[i])\n",
    "c,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0 c =  1.0 h =  0.7615941559557649\n",
      "i =  1 c =  -1.0 h =  -0.7615941559557649\n",
      "i =  2 c =  1.0 h =  0.3807970779778824\n",
      "i =  3 c =  0.9999999999114485 h =  0.7615941559185755\n",
      "i =  4 c =  -1.0 h =  -0.7615941559557649\n"
     ]
    }
   ],
   "source": [
    "x_all = np.array([1,1,0,1,1])\n",
    "h = 0\n",
    "c = 0\n",
    "for i in np.arange(5):\n",
    "    c, h = cal(c, h, x_all[i])\n",
    "    print('i = ', i, 'c = ', c, 'h = ', h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return max(0, Z)\n",
    "\n",
    "def simple_network():\n",
    "    t = 1\n",
    "    x = 3\n",
    "    w1 = 0.01\n",
    "    w2 = -5\n",
    "    b = -1\n",
    "    \n",
    "    z1 = w1 * x\n",
    "    a1 = relu(z1)\n",
    "    z2 = w2 * a1 + b\n",
    "    y = sigmoid(z2)\n",
    "    C = 0.5 * (y-t)**2\n",
    "    \n",
    "    ans = \"z1 = {}, a1 = {}, z2 = {}, y = {}, C = {}\".format(z1,a1,z2,y,C)\n",
    "    print(ans)\n",
    "    \n",
    "    dc_dw1 = (y-t) * dy_dz2(z2) * w2 * da1_dz1(z1) * x\n",
    "    print('dc_dw1 =', dc_dw1)\n",
    "    \n",
    "    dc_dw2 = (y-t) * dy_dz2(z2) *a1\n",
    "    print('dc_dw2 = ', dc_dw2)\n",
    "    \n",
    "    dc_db = (y-t) * dy_dz2(z2) *1\n",
    "    print('dc_db = ', dc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1 = 0.03, a1 = 0.03, z2 = -1.15, y = 0.24048908305088898, C = 0.28842841648243966\n",
      "dc_dw1 = 2.080916562170455\n",
      "dc_dw2 =  -0.004161833124340909\n",
      "dc_db =  -0.13872777081136364\n"
     ]
    }
   ],
   "source": [
    "simple_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_dy(Y,T):\n",
    "    return Y-T\n",
    "\n",
    "def dy_dz2(Z):\n",
    "    val = np.exp(Z)\n",
    "    return val/(1+val)**2\n",
    "\n",
    "def dz2_w2(A):\n",
    "    return A\n",
    "\n",
    "def dz2_a1(W):\n",
    "    return W\n",
    "\n",
    "def dz2_b(B):\n",
    "    return 1\n",
    "\n",
    "def da1_dz1(Z):\n",
    "    if Z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def dz1_w1(X):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([2, 1]), array([5, 8]), array([2, 5, 8, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = np.array([1,2,3])\n",
    "g1 = np.array([2,1])\n",
    "g2 = np.convolve(f1,g1,mode='valid')\n",
    "g3 = np.convolve(f1,g1,mode='full')\n",
    "f1,g1,g2,g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -3,  2,  2,  2,  1, -3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = np.array([1,3,-1,1,-3])\n",
    "g1 = np.array([-1,0,1])\n",
    "g2 = np.convolve(f1,g1,mode='full')\n",
    "g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function correlate in module numpy:\n",
      "\n",
      "correlate(a, v, mode='valid')\n",
      "    Cross-correlation of two 1-dimensional sequences.\n",
      "    \n",
      "    This function computes the correlation as generally defined in signal\n",
      "    processing texts::\n",
      "    \n",
      "        c_{av}[k] = sum_n a[n+k] * conj(v[n])\n",
      "    \n",
      "    with a and v sequences being zero-padded where necessary and conj being\n",
      "    the conjugate.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a, v : array_like\n",
      "        Input sequences.\n",
      "    mode : {'valid', 'same', 'full'}, optional\n",
      "        Refer to the `convolve` docstring.  Note that the default\n",
      "        is 'valid', unlike `convolve`, which uses 'full'.\n",
      "    old_behavior : bool\n",
      "        `old_behavior` was removed in NumPy 1.10. If you need the old\n",
      "        behavior, use `multiarray.correlate`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        Discrete cross-correlation of `a` and `v`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    convolve : Discrete, linear convolution of two one-dimensional sequences.\n",
      "    multiarray.correlate : Old, no conjugate, version of correlate.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The definition of correlation above is not unique and sometimes correlation\n",
      "    may be defined differently. Another common definition is::\n",
      "    \n",
      "        c'_{av}[k] = sum_n a[n] conj(v[n+k])\n",
      "    \n",
      "    which is related to ``c_{av}[k]`` by ``c'_{av}[k] = c_{av}[-k]``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.correlate([1, 2, 3], [0, 1, 0.5])\n",
      "    array([3.5])\n",
      "    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\")\n",
      "    array([2. ,  3.5,  3. ])\n",
      "    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\")\n",
      "    array([0.5,  2. ,  3.5,  3. ,  0. ])\n",
      "    \n",
      "    Using complex sequences:\n",
      "    \n",
      "    >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')\n",
      "    array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n",
      "    \n",
      "    Note that you get the time reversed, complex conjugated result\n",
      "    when the two input sequences change places, i.e.,\n",
      "    ``c_{va}[k] = c^{*}_{av}[-k]``:\n",
      "    \n",
      "    >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\n",
      "    array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.correlate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([1, 2]), array([2, 5, 8, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = np.array([1,2,3])\n",
    "g1 = np.array([1,2])\n",
    "h = np.correlate(f1,g1,\"full\")\n",
    "f1,g1,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 2, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "g1 = np.array([[1,0],[0,1]])\n",
    "h = signal.correlate2d(f1,g1,\"full\")\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5030], requires_grad=True) tensor([0.5391], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_a = torch.randn(1, requires_grad=True)\n",
    "x_b = torch.randn(1, requires_grad=True)\n",
    "print(x_a, x_b)\n",
    "x = x_a * x_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5427,  0.4848],\n",
       "        [ 0.4238, -0.4398],\n",
       "        [ 1.2818,  0.2862],\n",
       "        [-2.4152, -0.2637],\n",
       "        [-0.0614, -0.1718],\n",
       "        [ 0.1591,  0.4193],\n",
       "        [ 2.0659, -1.9378],\n",
       "        [-1.6786,  0.3529],\n",
       "        [-0.6918,  0.1785],\n",
       "        [ 1.0274, -0.9793]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = torch.randn(10, 2, requires_grad=True)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(10,10)\n",
    "print(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(1,20,5,1)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "(3, 2) (1, 3) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "input_to_hidden_weights = np.matrix('1. 1.; 1. 1.; 1. 1.')\n",
    "hidden_to_output_weights = np.matrix('1. 1. 1.')\n",
    "biases = np.matrix('0.; 0.; 0.')\n",
    "learning_rate = .001\n",
    "epochs_to_train = 10\n",
    "training_points = [((2,1), 10), ((3,3), 21), ((4,5), 32), ((6, 6), 42)]\n",
    "print(input_to_hidden_weights)\n",
    "print(input_to_hidden_weights.shape, hidden_to_output_weights.shape, hidden_to_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit(x):\n",
    "    \"\"\" Returns the ReLU of x, or the maximum between 0 and x.\"\"\"\n",
    "    # TODO\n",
    "    return max(0, x)\n",
    "\n",
    "def rectified_linear_unit_derivative(x):\n",
    "    \"\"\" Returns the derivative of ReLU.\"\"\"\n",
    "    # TODO\n",
    "    if (x <= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def output_layer_activation(x):\n",
    "    \"\"\" Linear function, returns input as is. \"\"\"\n",
    "    return x\n",
    "\n",
    "def output_layer_activation_derivative(x):\n",
    "    \"\"\" Returns the derivative of a linear function: 1. \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1 = [[2]\n",
      " [1]] \n",
      "input_to_hidden_weights = [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "hidden_layer_weighted_input = [[3.]\n",
      " [3.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "#temp1 = np.matrix([[x1],[x2]])\n",
    "temp1 = np.array([x1,x2]).reshape(2,1)\n",
    "hidden_layer_weighted_input = np.matmul(input_to_hidden_weights,(temp1)) + \\\n",
    "                                        biases # TODO (3 by 1 matrix)\n",
    "result_str = \"temp1 = {} \\ninput_to_hidden_weights = {} \\nhidden_layer_weighted_input = {}\".format(\\\n",
    "                                                        temp1, input_to_hidden_weights, hidden_layer_weighted_input)\n",
    "print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1 =  [[2]\n",
      " [1]]\n",
      "Z = [[3.]\n",
      " [3.]\n",
      " [3.]], \n",
      "f(Z) = [[3.]\n",
      " [3.]\n",
      " [3.]], \n",
      "u1 = [[9.]], \n",
      "f(u1) = [[9.]]\n"
     ]
    }
   ],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "#temp1 = np.matrix([[x1],[x2]])\n",
    "temp1 = np.array([x1,x2]).reshape(2,1)\n",
    "\n",
    "print('temp1 = ', temp1)\n",
    "hidden_layer_weighted_input = np.matmul(input_to_hidden_weights, temp1) + \\\n",
    "                                        biases # TODO (3 by 1 matrix)\n",
    "v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input)# TODO (3 by 1 matrix)\n",
    "\n",
    "output =  np.matmul(hidden_to_output_weights, hidden_layer_activation)# TODO\n",
    "v_output_layer_activation = np.vectorize(output_layer_activation)\n",
    "activated_output = v_output_layer_activation(output)# TODO\n",
    "result_str = \"Z = {}, \\nf(Z) = {}, \\nu1 = {}, \\nf(u1) = {}\".format(hidden_layer_weighted_input, \\\n",
    "                                                            hidden_layer_activation, output, activated_output)\n",
    "print(result_str)\n",
    "#txt3 = \"My name is {}, I'm {}\".format(\"John\",36)\n",
    "output_layer_error = (activated_output - y) * output_layer_activation_derivative(output) # TODO\n",
    "v_rectified_linear_unit_derivative = np.vectorize(rectified_linear_unit_derivative)\n",
    "hidden_layer_error = output_layer_error * hidden_to_output_weights # TODO (3 by 1 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "temp1 = np.matrix([[x1],[x2]])\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1 =  [[2]\n",
      " [1]]\n",
      "Z = [[3.]\n",
      " [3.]\n",
      " [3.]], \n",
      "f(Z) = [[3.]\n",
      " [3.]\n",
      " [3.]], \n",
      "u1 = [[9.]], \n",
      "f(u1) = [[9.]]\n",
      "(3, 1) (1, 3)\n",
      "output_layer_error = [[-1.]], \n",
      "hidden_layer_error = [[-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "bias_gradients = [[-1.]\n",
      " [-1.]\n",
      " [-1.]] \n",
      "hidden_to_output_weight_gradients = [[-3.]\n",
      " [-3.]\n",
      " [-3.]]                             \n",
      "input_to_hidden_weight_gradients = [[-2. -1.]\n",
      " [-2. -1.]\n",
      " [-2. -1.]]\n",
      "new values: \n",
      "biases = [[1.]\n",
      " [1.]\n",
      " [1.]], input_to_hidden_weights = [[3. 2.]\n",
      " [3. 2.]\n",
      " [3. 2.]], hidden_to_output_weights = [[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "#temp1 = np.matrix([[x1],[x2]])\n",
    "temp1 = np.array([x1,x2]).reshape(2,1)\n",
    "\n",
    "print('temp1 = ', temp1)\n",
    "hidden_layer_weighted_input = np.matmul(input_to_hidden_weights, temp1) + \\\n",
    "                                        biases # TODO (3 by 1 matrix)\n",
    "v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input)# TODO (3 by 1 matrix)\n",
    "\n",
    "output =  np.matmul(hidden_to_output_weights, hidden_layer_activation)# TODO\n",
    "v_output_layer_activation = np.vectorize(output_layer_activation)\n",
    "activated_output = v_output_layer_activation(output)# TODO\n",
    "result_str = \"Z = {}, \\nf(Z) = {}, \\nu1 = {}, \\nf(u1) = {}\".format(hidden_layer_weighted_input, \\\n",
    "                                                            hidden_layer_activation, output, activated_output)\n",
    "print(result_str)\n",
    "#txt3 = \"My name is {}, I'm {}\".format(\"John\",36)\n",
    "output_layer_error = (activated_output - y) * output_layer_activation_derivative(output) # TODO\n",
    "v_rectified_linear_unit_derivative = np.vectorize(rectified_linear_unit_derivative)\n",
    "hidden_layer_error = output_layer_error * hidden_to_output_weights # TODO (3 by 1 matrix)\n",
    "print(hidden_layer_activation.shape, hidden_layer_error.shape)\n",
    "hidden_layer_error = np.multiply(v_rectified_linear_unit_derivative(hidden_layer_activation), \\\n",
    "                                 np.transpose(hidden_layer_error))\n",
    "result_str = \"output_layer_error = {}, \\nhidden_layer_error = {}\".format(output_layer_error, hidden_layer_error)\n",
    "print(result_str)\n",
    "\n",
    "bias_gradients = hidden_layer_error # TODO\n",
    "hidden_to_output_weight_gradients = hidden_layer_activation * output_layer_error# TODO\n",
    "input_to_hidden_weight_gradients = hidden_layer_error *np.transpose(input_values)# TODO\n",
    "result_str = \"bias_gradients = {} \\nhidden_to_output_weight_gradients = {} \\\n",
    "                            \\ninput_to_hidden_weight_gradients = {}\".format(bias_gradients, \\\n",
    "                                                                hidden_to_output_weight_gradients, \\\n",
    "                                                                input_to_hidden_weight_gradients)\n",
    "print(result_str)\n",
    "\n",
    "# Use gradients to adjust weights and biases using gradient descent\n",
    "biases = biases - bias_gradients # TODO\n",
    "input_to_hidden_weights = input_to_hidden_weights - input_to_hidden_weight_gradients# TODO\n",
    "hidden_to_output_weights = hidden_to_output_weights - hidden_to_output_weight_gradients# TODO\n",
    "result_str = \"new values: \\nbiases = {}, input_to_hidden_weights = {}, hidden_to_output_weights = {}\".\\\n",
    "            format(biases, input_to_hidden_weights, hidden_to_output_weights)\n",
    "print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.]] [[9.]]\n",
      "output_layer_erro =  [[-1.]]\n",
      "hidden_layer_error =  [[0 0 0]]\n",
      "hidden_to_output_weight_gradients =  [[-3.]\n",
      " [-3.]\n",
      " [-3.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fc1709c8b086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mhidden_to_output_weight_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer_activation\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutput_layer_error\u001b[0m\u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hidden_to_output_weight_gradients = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_to_output_weight_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minput_to_hidden_weight_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_values\u001b[0m\u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "temp1 = np.matrix([[x1],[x2]])\n",
    "hidden_layer_weighted_input = np.matmul(input_to_hidden_weights, input_values) + \\\n",
    "                                        biases # TODO (3 by 1 matrix)\n",
    "v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input)# TODO (3 by 1 matrix)\n",
    "\n",
    "output =  np.matmul(hidden_to_output_weights, hidden_layer_activation)# TODO\n",
    "v_output_layer_activation = np.vectorize(output_layer_activation)\n",
    "activated_output = v_output_layer_activation(output)# TODO\n",
    "\n",
    "### Backpropagation ###\n",
    "\n",
    "# Compute gradients\n",
    "output_layer_error = (activated_output - y) * output_layer_activation_derivative(output) # TODO\n",
    "v_rectified_linear_unit_derivative = np.vectorize(rectified_linear_unit_derivative)\n",
    "hidden_layer_error = output_layer_error * hidden_to_output_weights # TODO (3 by 1 matrix)\n",
    "\n",
    "bias_gradients = hidden_layer_error # TODO\n",
    "hidden_to_output_weight_gradients = hidden_layer_activation * output_layer_error# TODO\n",
    "input_to_hidden_weight_gradients = \\\n",
    "                                    v_rectified_linear_unit_derivative(hidden_layer_error) * \\\n",
    "                                    input_values# TODO\n",
    "\n",
    "# Use gradients to adjust weights and biases using gradient descent\n",
    "biases = biases - bias_gradients # TODO\n",
    "input_to_hidden_weights = input_to_hidden_weights - input_to_hidden_weight_gradients# TODO\n",
    "hidden_to_output_weights = hidden_to_output_weights - hidden_to_output_weight_gradients# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 =2\n",
    "x2 = 1\n",
    "y = 10\n",
    "temp1 = np.matrix([[x1],[x2]])\n",
    "hidden_layer_weighted_input = np.matmul(input_to_hidden_weights, temp1)\n",
    "v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input)\n",
    "\n",
    "output = np.matmul(hidden_to_output_weights, hidden_layer_activation)\n",
    "v_output_layer_activation = np.vectorize(output_layer_activation)\n",
    "activated_output = v_output_layer_activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0.5]]), matrix([[9.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_error = 0.5 * (y-activated_output)**2\n",
    "output_layer_error, activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19661193324148185"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.exp(1)\n",
    "n1/(1+n1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x1, x2, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "    input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "    hidden_layer_weighted_input = # TODO (3 by 1 matrix)\n",
    "    hidden_layer_activation = # TODO (3 by 1 matrix)\n",
    "\n",
    "    output =  # TODO\n",
    "    activated_output = # TODO\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "    output_layer_error = # TODO\n",
    "    hidden_layer_error = # TODO (3 by 1 matrix)\n",
    "\n",
    "    bias_gradients = # TODO\n",
    "    hidden_to_output_weight_gradients = # TODO\n",
    "    input_to_hidden_weight_gradients = # TODO\n",
    "\n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "    biases = # TODO\n",
    "    input_to_hidden_weights = # TODO\n",
    "    hidden_to_output_weights = # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
